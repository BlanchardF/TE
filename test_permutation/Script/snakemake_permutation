#Dry run
#nohup /home/data/fblanchard/miniconda3/envs/snakemake/bin/snakemake -s snakemake_permutation -n &> nohup_snakemake_permutation &

#Unlock
#nohup /home/data/fblanchard/miniconda3/envs/snakemake/bin/snakemake -s snakemake_permutation --unlock  &> nohup_snakemake_permutation &

#Realcommand
#nohup /home/data/fblanchard/miniconda3/envs/snakemake/bin/snakemake -s snakemake_permutation --cores 16 &> nohup_snakemake_permutation &

############################################
##                CONFIG                  ##
############################################

Version = "All"

Data = f"/home/data/fblanchard/TE/test_permutation/Data/{Version}" 
Results = f"/home/data/fblanchard/TE/test_permutation/Results/{Version}"

Genome = "/home/data/fblanchard/TE/test_permutation/Data" 

Lignee = f"{Data}/Lignee"
Region = f"{Data}/Region"

import os

# Lignées : fichiers BED à permuter
lignee_ids = [
    os.path.splitext(f)[0]
    for f in os.listdir(Lignee)
    if f.endswith(".bed")
]

# Paramètre : nombre de permutations
n_permutations = 1000

# Liste ordonnée des fichiers B (régions de comparaison)
B_FILES = [
    "/home/data/alarue/rasusa_35x/TrEMOLO/against_G0/Wb+_data/gencode/promoters.2000_filtered_cleaned_corrected.bed",
    "/home/data/alarue/rasusa_35x/TrEMOLO/against_G0/Wb+_data/gencode/5UTRs_filtered_cleaned.bed",
    "/home/data/alarue/rasusa_35x/TrEMOLO/against_G0/Wb+_data/gencode/3UTRs_filtered_cleaned.bed",
    "/home/data/alarue/rasusa_35x/TrEMOLO/against_G0/Wb+_data/gencode/exons_filtered_cleaned.bed",
    "/home/data/alarue/rasusa_35x/TrEMOLO/against_G0/Wb+_data/gencode/introns_filtered_cleaned.bed"
]
B_NAMES = [os.path.splitext(os.path.basename(f))[0] for f in B_FILES]


############################################
##                RULES                   ##
############################################

rule all:
    input:
        expand(f"{Results}/{{sample}}/ordered_intersections/summary.txt", sample=lignee_ids)


rule get_missing_contigs:
    input:
        bed = f"{Lignee}/{{sample}}.bed",
        ref = f"{Genome}/G0_Wb_3seq_rmdup_ragtag.scaffold.genome"
    output:
        missing = f"{Results}/{{sample}}/missing_contigs.txt",
    shell:
        """
        mkdir -p $(dirname {output.missing})
        cut -f1 {input.bed} | sort | uniq > {output.missing}.tmp_bed_contigs
        cut -f1 {input.ref} | sort | uniq > {output.missing}.tmp_ref_contigs
        comm -23 {output.missing}.tmp_bed_contigs {output.missing}.tmp_ref_contigs > {output.missing}
        rm {output.missing}.tmp_*
        """


rule filter_bed_on_contigs:
    input:
        bed = f"{Lignee}/{{sample}}.bed",
        missing = f"{Results}/{{sample}}/missing_contigs.txt"
    output:
        filtered = f"{Results}/{{sample}}/filtered.bed"
    shell:
        """
        if [ -s {input.missing} ]; then
            grep -v -F -f {input.missing} {input.bed} | cut -f1-6 > {output.filtered}
        else
            cut -f1-6 {input.bed} > {output.filtered}
        fi
        """


rule generate_shuffles:
    input:
        filtered = f"{Results}/{{a}}/filtered.bed",
        genome = f"{Genome}/G0_Wb_3seq_rmdup_ragtag.scaffold.genome"
    output:
        shuffled = f"{Results}/{{a}}/shuffles.bed"
    params:
        n = n_permutations
    shell:
        """
        mkdir -p $(dirname {output.shuffled})
        > {output.shuffled}
        for i in $(seq 1 {params.n}); do
            bedtools shuffle -i {input.filtered} -g {input.genome} -noOverlapping -seed $i  \
            | awk -v p=$i '{{print $0"\t"p}}' >> {output.shuffled}
        done
        #-chrom
        """


rule ordered_intersect:
    input:
        shuffled = f"{Results}/{{a}}/shuffles.bed",
        filtered = f"{Results}/{{a}}/filtered.bed"
    output:
        summary = f"{Results}/{{a}}/ordered_intersections/summary.txt"
    run:
        import os
        import subprocess
        import pandas as pd
        import numpy as np

        outdir = os.path.dirname(output.summary)
        os.makedirs(outdir, exist_ok=True)

        # --- 1. Charger permutations ---
        df = pd.read_csv(input.shuffled, sep="\t", header=None,
                         names=['chrom','start','end','name','score','strand','perm'])
        current_df = df.copy()

        perm_dists = {}   # distribution complète des intersections
        mean_counts = {}  # moyennes des permutations

        for bfile, bname in zip(B_FILES, B_NAMES):
            out_inter = os.path.join(outdir, f"{wildcards.a}_{bname}_intersected_perm.bed")
            out_sub = os.path.join(outdir, f"{wildcards.a}_{bname}_remaining_perm.bed")

            tmp_current = os.path.join(outdir, "tmp_current_perm.bed")
            current_df.to_csv(tmp_current, sep="\t", header=False, index=False)

            subprocess.check_call(f"bedtools intersect -wa -u -a {tmp_current} -b {bfile} > {out_inter}", shell=True)
            subprocess.check_call(f"bedtools intersect -v -a {tmp_current} -b {bfile} > {out_sub}", shell=True)

            # Charger intersections avec colonnes (inclut 'perm')
            df_inter = pd.read_csv(out_inter, sep="\t", header=None)

            if not df_inter.empty:
                # nb d’intersections par permutation
                counts = df_inter[6].value_counts().sort_index()
                # compléter pour toutes les permutations
                counts = counts.reindex(range(1, n_permutations+1), fill_value=0)
            else:
                counts = pd.Series(0, index=range(1, n_permutations+1))

            perm_dists[bname] = counts.values
            mean_counts[f"{bname}_mean_per_perm"] = counts.mean()

            current_df = pd.read_csv(out_sub, sep="\t", header=None)
            os.remove(tmp_current)

        # --- 2. VRAI BED ---
        true_counts = {}
        pvalues = {}
        current_bed = input.filtered

        for bfile, bname in zip(B_FILES, B_NAMES):
            out_inter_true = os.path.join(outdir, f"{wildcards.a}_{bname}_intersected_true.bed")
            out_sub_true = os.path.join(outdir, f"{wildcards.a}_{bname}_remaining_true.bed")

            subprocess.check_call(f"bedtools intersect -wa -u -a {current_bed} -b {bfile} > {out_inter_true}", shell=True)
            subprocess.check_call(f"bedtools intersect -v -a {current_bed} -b {bfile} > {out_sub_true}", shell=True)

            # Compter intersections réelles
            with open(out_inter_true) as f:
                n_lines = sum(1 for _ in f)

            true_counts[f"{bname}_true_count"] = n_lines

            # Comparer à la distribution permutée
            dist = perm_dists[bname]
            greater_equal = np.sum(dist >= n_lines)
            less_equal = np.sum(dist <= n_lines)

            p_over = (greater_equal + 1) / (n_permutations + 1)
            p_under = (less_equal + 1) / (n_permutations + 1)
            p_two = 2 * min(p_over, p_under)

            pvalues[f"{bname}_pvalue"] = min(1.0, p_two)  # clamp à 1.0

            current_bed = out_sub_true

        # --- 3. ÉCRITURE DU SUMMARY ---
        with open(output.summary, "w") as f:
            for bname in B_NAMES:
                f.write(f"{bname}\tmean_perm\t{mean_counts[f'{bname}_mean_per_perm']:.2f}\n")
                f.write(f"{bname}\ttrue_count\t{true_counts[f'{bname}_true_count']}\n")
                f.write(f"{bname}\tpvalue\t{pvalues[f'{bname}_pvalue']:.4f}\n")



# for file in ../Results/All/*/*/summary.txt; do   echo "=== $file ===";   cat "$file";   echo ""; done > ../Results/All/All_result_3.txt 